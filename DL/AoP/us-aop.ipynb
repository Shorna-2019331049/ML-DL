{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4411d4c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-08T06:24:29.459381Z",
     "iopub.status.busy": "2025-04-08T06:24:29.459055Z",
     "iopub.status.idle": "2025-04-08T06:26:30.939352Z",
     "shell.execute_reply": "2025-04-08T06:26:30.938144Z"
    },
    "papermill": {
     "duration": 121.490196,
     "end_time": "2025-04-08T06:26:30.941131",
     "exception": false,
     "start_time": "2025-04-08T06:24:29.450935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/final-masks/Pred_Images_for_TN_1463-20250328T162926Z-001/Pred_Images_for_TN_1463/Unet_1463/__huggingface_repos__.json\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1463-20250328T162926Z-001/Pred_Images_for_TN_1463/Unet_1463/Unet/predicted_masks/pred_20191129T103133_749_mask.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1463-20250328T162926Z-001/Pred_Images_for_TN_1463/Unet_1463/__results___files/__results___5_0.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1463-20250328T162926Z-001/Pred_Images_for_TN_1463/TransUnet_1463/__huggingface_repos__.json\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1463-20250328T162926Z-001/Pred_Images_for_TN_1463/TransUnet_1463/TransUnet/predicted_masks/pred_20191129T103133_749_mask.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1463-20250328T162926Z-001/Pred_Images_for_TN_1463/TransUnet_1463/__results___files/__results___5_0.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1463-20250328T162926Z-001/Pred_Images_for_TN_1463/TransFuse_1463/__huggingface_repos__.json\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1463-20250328T162926Z-001/Pred_Images_for_TN_1463/TransFuse_1463/TransFuse/predicted_masks/pred_20191129T103133_749_mask.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1463-20250328T162926Z-001/Pred_Images_for_TN_1463/TransFuse_1463/__results___files/__results___5_0.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1463-20250328T162926Z-001/Pred_Images_for_TN_1463/AttentionUnet_1463/__huggingface_repos__.json\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1463-20250328T162926Z-001/Pred_Images_for_TN_1463/AttentionUnet_1463/__results___files/__results___5_0.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1463-20250328T162926Z-001/Pred_Images_for_TN_1463/AttentionUnet_1463/AttentionUnet/predicted_masks/pred_20191129T103133_749_mask.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1463-20250328T162926Z-001/Pred_Images_for_TN_1463/medT_1463/__huggingface_repos__.json\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1463-20250328T162926Z-001/Pred_Images_for_TN_1463/medT_1463/medT/predicted_masks/pred_20191129T103133_749_mask.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1463-20250328T162926Z-001/Pred_Images_for_TN_1463/medT_1463/__results___files/__results___5_0.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1463-20250328T162926Z-001/Pred_Images_for_TN_1463/RAUnet_1463/__huggingface_repos__.json\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1463-20250328T162926Z-001/Pred_Images_for_TN_1463/RAUnet_1463/RAUnet/predicted_masks/pred_20191129T103133_749_mask.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1463-20250328T162926Z-001/Pred_Images_for_TN_1463/RAUnet_1463/__results___files/__results___5_0.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/unet_1414/__huggingface_repos__.json\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/unet_1414/Unet/predicted_masks/pred_20200103T102623_269_mask.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/unet_1414/__results___files/__results___5_0.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/medT_1414/__huggingface_repos__.json\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/medT_1414/medT/predicted_masks/pred_20200103T102623_269_mask.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/medT_1414/__results___files/__results___5_0.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/TransFuse_1414/__huggingface_repos__.json\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/TransFuse_1414/TransFuse/predicted_masks/pred_20200103T102623_269_mask.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/TransFuse_1414/__results___files/__results___5_0.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/RAUnet_1414/__huggingface_repos__.json\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/RAUnet_1414/RAUnet/predicted_masks/pred_20200103T102623_269_mask.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/RAUnet_1414/__results___files/__results___5_0.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/TransUnet_1414/__huggingface_repos__.json\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/TransUnet_1414/TransUnet/predicted_masks/pred_20200103T102623_269_mask.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/TransUnet_1414/__results___files/__results___5_0.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/AttentionUnet_1414/__huggingface_repos__.json\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/AttentionUnet_1414/__results___files/__results___5_0.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/AttentionUnet_1414/AttentionUnet/predicted_masks/pred_20200103T102623_269_mask.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1470-20250328T201100Z-001/Pred_Images_for_TN_1470/MedT_1470/__huggingface_repos__.json\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1470-20250328T201100Z-001/Pred_Images_for_TN_1470/MedT_1470/medT/predicted_masks/pred_20191129T103133_749_mask.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1470-20250328T201100Z-001/Pred_Images_for_TN_1470/MedT_1470/__results___files/__results___5_0.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1470-20250328T201100Z-001/Pred_Images_for_TN_1470/Raunet_1470/__huggingface_repos__.json\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1470-20250328T201100Z-001/Pred_Images_for_TN_1470/Raunet_1470/RAUnet/predicted_masks/pred_20191129T103133_749_mask.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1470-20250328T201100Z-001/Pred_Images_for_TN_1470/Raunet_1470/__results___files/__results___5_0.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1470-20250328T201100Z-001/Pred_Images_for_TN_1470/Unet_1470/__huggingface_repos__.json\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1470-20250328T201100Z-001/Pred_Images_for_TN_1470/Unet_1470/Unet/predicted_masks/pred_20191129T103133_749_mask.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1470-20250328T201100Z-001/Pred_Images_for_TN_1470/Unet_1470/__results___files/__results___5_0.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1470-20250328T201100Z-001/Pred_Images_for_TN_1470/Attention_unet_1470/__huggingface_repos__.json\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1470-20250328T201100Z-001/Pred_Images_for_TN_1470/Attention_unet_1470/__results___files/__results___5_0.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1470-20250328T201100Z-001/Pred_Images_for_TN_1470/Attention_unet_1470/AttentionUnet/predicted_masks/pred_20191129T103133_749_mask.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1470-20250328T201100Z-001/Pred_Images_for_TN_1470/transunet_1470/__huggingface_repos__.json\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1470-20250328T201100Z-001/Pred_Images_for_TN_1470/transunet_1470/TransUnet/predicted_masks/pred_20191129T103133_749_mask.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1470-20250328T201100Z-001/Pred_Images_for_TN_1470/transunet_1470/__results___files/__results___5_0.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1470-20250328T201100Z-001/Pred_Images_for_TN_1470/Transfuse_1470/__huggingface_repos__.json\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1470-20250328T201100Z-001/Pred_Images_for_TN_1470/Transfuse_1470/TransFuse/predicted_masks/pred_20191129T103133_749_mask.png\n",
      "/kaggle/input/final-masks/Pred_Images_for_TN_1470-20250328T201100Z-001/Pred_Images_for_TN_1470/Transfuse_1470/__results___files/__results___5_0.png\n",
      "/kaggle/input/label-mask2/__results___files/__results___1_0.png\n",
      "/kaggle/input/label-mask2/predicted_masks/pred_20200103T102623_269_mask.png\n",
      "/kaggle/input/unetdata/denoised-20250123T134822Z-001/denoised/Standard_plane/20191115T105730_719.png\n",
      "/kaggle/input/unetdata/denoised-20250123T134822Z-001/denoised/Non-standard_plane/20191220T102712_59.png\n",
      "/kaggle/input/unetdata/TN_images-20250123T134827Z-001/TN_images/01580PSFHSAoP4k_1580.png\n",
      "/kaggle/input/unetdata/test_Masks-20250123T134752Z-001/test_Masks/00844_PSFHAoP4k_mask.png\n",
      "/kaggle/input/unetdata/Filtered_Masks-20250123T134808Z-001/Filtered_Masks/20191026T195815_539_mask.png\n",
      "/kaggle/input/ground-truth-mask-img/testMaskResnet18Cbam-20250329T151022Z-001/testMaskResnet18Cbam/20191026T195815_409_mask.png\n",
      "/kaggle/input/ground-truth-mask-img/test_Masks-20250329T151233Z-001/test_Masks/00844_PSFHAoP4k_mask.png\n",
      "/kaggle/input/ground-truth-mask-img/TN_Mask_1463-20250329T151137Z-001/TN_Mask_1463/20191026T195815_409_mask.png\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        break\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d816377",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:26:30.958339Z",
     "iopub.status.busy": "2025-04-08T06:26:30.957823Z",
     "iopub.status.idle": "2025-04-08T06:26:31.299770Z",
     "shell.execute_reply": "2025-04-08T06:26:31.298844Z"
    },
    "papermill": {
     "duration": 0.352378,
     "end_time": "2025-04-08T06:26:31.301656",
     "exception": false,
     "start_time": "2025-04-08T06:26:30.949278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def find_tangent_lines(ellipse, reference_point):\n",
    "    \"\"\"Find the Ellipse's two tangents that go through a reference point\n",
    "\n",
    "    Args:\n",
    "        ellipse:\n",
    "          center: The center of the ellipse\n",
    "          axes: Major and Minor axes of the ellipse\n",
    "          rotation: The counter-clockwise rotation of the ellipse in radians\n",
    "        reference_point: The coordinates of the reference point.\n",
    "\n",
    "    Return:\n",
    "        (m1, h1): Slope and intercept of the first tangent.\n",
    "        (m2, h2): Slope and intercept of the second tangent.\n",
    "    \"\"\"\n",
    "    (x0, y0), axes, rotation  = ellipse\n",
    "    a, b = axes[0]/2, axes[1]/2\n",
    "    rotation =  np.radians(rotation)\n",
    "    s, c = np.sin(rotation), np.cos(rotation)\n",
    "    p0, q0 = reference_point\n",
    "\n",
    "    A = (-a ** 2 * s ** 2 - b ** 2 * c ** 2 + (y0 - q0) ** 2)\n",
    "    B = 2 * (c * s * (a ** 2 - b ** 2) - (x0 - p0) * (y0 - q0))\n",
    "    C = (-a ** 2 * c ** 2 - b ** 2 * s ** 2 + (x0 - p0) ** 2)\n",
    "\n",
    "    if B ** 2 - 4 * A * C < 0:\n",
    "        raise ValueError('Reference point lies inside the ellipse')\n",
    "\n",
    "    t1, t2 = (\n",
    "        (-B + np.sqrt(B ** 2 - 4 * A * C)) / (2 * A),\n",
    "        (-B - np.sqrt(B ** 2 - 4 * A * C)) / (2 * A),\n",
    "    )\n",
    "    return (\n",
    "        (1 / t1, q0 - p0 / t1),\n",
    "        (1 / t2, q0 - p0 / t2),\n",
    "    )\n",
    "\n",
    "\n",
    "# Coordinates of the ellipse's major axes\n",
    "def get_major_axis(ellipse):\n",
    "\n",
    "    center, axes, angle = ellipse\n",
    "    major_axis_angle_rad = np.radians(angle)\n",
    "    major_axis_length = max(axes)\n",
    "    cos_angle = np.cos(major_axis_angle_rad)\n",
    "    sin_angle = np.sin(major_axis_angle_rad)\n",
    "\n",
    "    major_axis_endpoint1 = (\n",
    "        int(center[0] - 0.5 * major_axis_length * sin_angle),\n",
    "        int(center[1] + 0.5 * major_axis_length * cos_angle)\n",
    "    )\n",
    "    major_axis_endpoint2 = (\n",
    "        int(center[0] + 0.5 * major_axis_length * sin_angle),\n",
    "        int(center[1] - 0.5 * major_axis_length * cos_angle)\n",
    "    )\n",
    "\n",
    "    return major_axis_endpoint1, major_axis_endpoint2\n",
    "\n",
    "# Angle between 2 lines\n",
    "def calculate_angle(lineA, lineB):\n",
    "\n",
    "    vA = np.array([(lineA[0][0] - lineA[1][0]), (lineA[0][1] - lineA[1][1])])\n",
    "    vB = np.array([(lineB[0][0] - lineB[1][0]), (lineB[0][1] - lineB[1][1])])\n",
    "\n",
    "    angle = np.arccos(np.dot(vA, vB) / np.dot(vA, vA) ** 0.5 / np.dot(vB, vB) ** 0.5)\n",
    "    deg = np.rad2deg(angle) \n",
    "\n",
    "    if deg - 180 >= 0:\n",
    "        return 360 - deg\n",
    "    else:\n",
    "        return deg\n",
    "        \n",
    "        \n",
    "def angle_of_progression_estimation(label, return_img=False):\n",
    "    tmp_img = np.zeros_like(label).astype(np.uint8)\n",
    "    \n",
    "    # PS ellipse\n",
    "    contours_ps, _ = cv2.findContours((label.cpu().numpy() == 1).astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if len(contours_ps) == 0:\n",
    "        print(\"Skipping PS contour due to no contours found.\")\n",
    "        return (None, None) if return_img else None\n",
    "\n",
    "    largest_contour_ps = max(contours_ps, key=cv2.contourArea)\n",
    "\n",
    "    if len(largest_contour_ps) < 5:\n",
    "        print(\"Skipping PS contour due to insufficient points.\")\n",
    "        return (None, None) if return_img else None\n",
    "\n",
    "    ellipse_ps = cv2.fitEllipse(largest_contour_ps)\n",
    "    \n",
    "    # PS ellipse: major axis points\n",
    "    (x1, y1), (x2, y2) = get_major_axis(ellipse_ps)\n",
    "    ps_points = [(x1, y1), (x2, y2)] if x1 > x2 else [(x2, y2), (x1, y1)]\n",
    "    ps_pont = (int(ps_points[0][0]), int(ps_points[0][1]))\n",
    "    \n",
    "    # FH ellipse\n",
    "    contours_fh, _ = cv2.findContours((label.cpu().numpy() == 2).astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if len(contours_fh) == 0:\n",
    "        print(\"Skipping FH contour due to no contours found.\")\n",
    "        return (None, None) if return_img else None\n",
    "\n",
    "    largest_contour_fh = max(contours_fh, key=cv2.contourArea)\n",
    "\n",
    "    if len(largest_contour_fh) < 5:\n",
    "        print(\"Skipping FH contour due to insufficient points.\")\n",
    "        return (None, None) if return_img else None\n",
    "\n",
    "    ellipse_fh = cv2.fitEllipse(largest_contour_fh)\n",
    "\n",
    "    # Find tangent line\n",
    "    try:\n",
    "        (m1, h1), (m2, h2) = find_tangent_lines(ellipse=ellipse_fh, reference_point=ps_pont)\n",
    "    except ValueError:\n",
    "        print(\"Skipping due to invalid tangent calculation.\")\n",
    "        return (None, None) if return_img else None\n",
    "\n",
    "    tmp_p_disp = 150\n",
    "    op = ((ps_points[0][1] + tmp_p_disp - h1) / m1)\n",
    "    \n",
    "    # Draw if needed\n",
    "    if return_img:\n",
    "        cv2.ellipse(tmp_img, ellipse_ps, (120, 255, 255), 1, cv2.LINE_AA) \n",
    "        cv2.line(tmp_img, (int(x1), int(y1)), (int(x2), int(y2)), (160, 0, 255), 2) \n",
    "        cv2.ellipse(tmp_img, ellipse_fh, (160, 255, 255), 1, cv2.LINE_AA)\n",
    "        cv2.line(tmp_img, ps_pont, (int(op), int(ps_points[0][1] + tmp_p_disp)), (240, 0, 0), 2)\n",
    "\n",
    "    # Angle estimation\n",
    "    aop = calculate_angle([ps_points[0], ps_points[1]], [ps_pont, (op, ps_points[0][1] + tmp_p_disp)])\n",
    "    \n",
    "    return (aop, tmp_img) if return_img else aop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35a88337",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:26:31.318567Z",
     "iopub.status.busy": "2025-04-08T06:26:31.318181Z",
     "iopub.status.idle": "2025-04-08T06:26:31.332638Z",
     "shell.execute_reply": "2025-04-08T06:26:31.331419Z"
    },
    "papermill": {
     "duration": 0.024654,
     "end_time": "2025-04-08T06:26:31.334227",
     "exception": false,
     "start_time": "2025-04-08T06:26:31.309573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matching masks: 1470\n",
      "GT: 04218PSFHSAoP1358_475_mask.png  -->  Pred: pred_04218PSFHSAoP1358_475_mask.png\n",
      "GT: 20191129T110732_739_mask.png  -->  Pred: pred_20191129T110732_739_mask.png\n",
      "GT: 01943PSFHSAoP4k_1943_mask.png  -->  Pred: pred_01943PSFHSAoP4k_1943_mask.png\n",
      "GT: 20200103T104919_649_mask.png  -->  Pred: pred_20200103T104919_649_mask.png\n",
      "GT: 20191220T112127_169_mask.png  -->  Pred: pred_20191220T112127_169_mask.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# Paths\n",
    "gt_path = \"/kaggle/input/ground-truth-mask-img/testMaskResnet18Cbam-20250329T151022Z-001/testMaskResnet18Cbam\"\n",
    "pred_path = \"/kaggle/input/final-masks/Pred_Images_for_TN_1470-20250328T201100Z-001/Pred_Images_for_TN_1470/Attention_unet_1470/AttentionUnet/predicted_masks\"\n",
    "\n",
    "# Get filenames\n",
    "gt_files = sorted(os.listdir(gt_path))\n",
    "pred_files = sorted(os.listdir(pred_path))\n",
    "\n",
    "# Remove \"pred_\" from predicted filenames\n",
    "clean_pred_files = [fname.replace(\"pred_\", \"\") for fname in pred_files]\n",
    "\n",
    "# Find common filenames\n",
    "common_images = list(set(gt_files) & set(clean_pred_files))\n",
    "\n",
    "print(f\"Total matching masks: {len(common_images)}\")  # Check how many match\n",
    "\n",
    "# Select random matching images\n",
    "num_images = min(5, len(common_images))  # Load up to 5\n",
    "random_images = random.sample(common_images, num_images)\n",
    "\n",
    "# Create matched pairs\n",
    "matched_pairs = [(img, f\"pred_{img}\") for img in random_images]\n",
    "\n",
    "# Print matched pairs for verification\n",
    "for gt_file, pred_file in matched_pairs:\n",
    "    print(f\"GT: {gt_file}  -->  Pred: {pred_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dd58d99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:26:31.351814Z",
     "iopub.status.busy": "2025-04-08T06:26:31.351410Z",
     "iopub.status.idle": "2025-04-08T06:26:31.358031Z",
     "shell.execute_reply": "2025-04-08T06:26:31.357136Z"
    },
    "papermill": {
     "duration": 0.016997,
     "end_time": "2025-04-08T06:26:31.359596",
     "exception": false,
     "start_time": "2025-04-08T06:26:31.342599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set output paths\n",
    "save_dir_working = '/kaggle/working/aop_visuals'\n",
    "save_dir_output = '/kaggle/outputs/aop_visuals'\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(save_dir_working, exist_ok=True)\n",
    "os.makedirs(save_dir_output, exist_ok=True)\n",
    "\n",
    "# Function to save image to both locations\n",
    "def save_aop_images(gt_name_base, gt_img, pred_img):\n",
    "    # Create base names\n",
    "    base_name = os.path.splitext(gt_name_base)[0]  # Remove .png\n",
    "    \n",
    "    # Construct full paths\n",
    "    gt_path_working = os.path.join(save_dir_working, f'{base_name}_gtAoP.png')\n",
    "    pred_path_working = os.path.join(save_dir_working, f'{base_name}_predAoP.png')\n",
    "\n",
    "    gt_path_output = os.path.join(save_dir_output, f'{base_name}_gtAoP.png')\n",
    "    pred_path_output = os.path.join(save_dir_output, f'{base_name}_predAoP.png')\n",
    "\n",
    "    # Save both images (GT and Pred)\n",
    "    cv2.imwrite(gt_path_working, gt_img)\n",
    "    cv2.imwrite(pred_path_working, pred_img)\n",
    "    cv2.imwrite(gt_path_output, gt_img)\n",
    "    cv2.imwrite(pred_path_output, pred_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b84f58ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:26:31.376676Z",
     "iopub.status.busy": "2025-04-08T06:26:31.376283Z",
     "iopub.status.idle": "2025-04-08T06:26:31.381090Z",
     "shell.execute_reply": "2025-04-08T06:26:31.380238Z"
    },
    "papermill": {
     "duration": 0.015133,
     "end_time": "2025-04-08T06:26:31.382618",
     "exception": false,
     "start_time": "2025-04-08T06:26:31.367485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import cv2  # Import OpenCV\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "\n",
    "# # Create matched pairs\n",
    "# matched_pairs = [(img, f\"pred_{img}\") for img in common_images]\n",
    "\n",
    "# meanAoP = []\n",
    "# skipped_images_count = 0\n",
    "# # Load and send images to function\n",
    "# for idx, (gt_name, pred_name) in enumerate(matched_pairs):\n",
    "#     # print(pred_name)\n",
    "\n",
    "#     # Read images with OpenCV\n",
    "#     gt_img = cv2.imread(os.path.join(gt_path, gt_name), 0)  # Load as grayscale\n",
    "#     pred_img = cv2.imread(os.path.join(pred_path, pred_name), 0)  # Load as grayscale\n",
    "    \n",
    "#     # Check if images are loaded properly\n",
    "#     if gt_img is None or pred_img is None:\n",
    "#         print(f\"Error loading images: {gt_name}, {pred_name}\")\n",
    "#         continue\n",
    "    \n",
    "#     gt_img = cv2.resize(gt_img, (224, 224), interpolation=cv2.INTER_NEAREST)  # Use INTER_NEAREST for categorical images\n",
    "#     pred_img = cv2.resize(pred_img, (224, 224), interpolation=cv2.INTER_NEAREST)  # Use INTER_NEAREST for categorical images\n",
    "\n",
    "#     # Convert to torch tensors\n",
    "#     pred_img = torch.from_numpy(pred_img).long()\n",
    "#     gt_img = torch.from_numpy(gt_img).long()\n",
    "\n",
    "#     # Print unique values in gt_img and pred_img\n",
    "#     # print(torch.unique(pred_img))\n",
    "    \n",
    "#     # Skip if the image contains only two unique classes\n",
    "#     if len(torch.unique(gt_img)) == 2 or len(torch.unique(pred_img)) == 2:\n",
    "#         skipped_images_count += 1\n",
    "#         continue\n",
    "\n",
    "#     # Define mapping: 0 → 0, 76 → 1, 127 → 1, 149 → 2, 255 → 2\n",
    "#     mapping_gt = {0: 0, 76: 1, 127: 1, 149: 2, 255: 2}\n",
    "#     mapping_pred = {0: 0, 127: 1, 255: 2}\n",
    "\n",
    "#     # Apply mapping using np.vectorize\n",
    "#     pred_img = np.vectorize(mapping_pred.get)(pred_img).astype(np.uint8)\n",
    "#     gt_img = np.vectorize(mapping_gt.get)(gt_img).astype(np.uint8)\n",
    "\n",
    "#     # Convert to torch tensors\n",
    "#     pred_img = torch.from_numpy(pred_img).long()\n",
    "#     gt_img = torch.from_numpy(gt_img).long()\n",
    "\n",
    "#     # Angle of Progression Estimation\n",
    "#     gtAoP = angle_of_progression_estimation(gt_img, return_img=True)\n",
    "#     predAoP = angle_of_progression_estimation(pred_img, return_img=True)\n",
    "\n",
    "#     # Check if either AoP value is None, and skip if true\n",
    "#     if gtAoP is None or predAoP is None:\n",
    "#         # print(f\"Skipping image pair {gt_name} and {pred_name} due to invalid AoP calculation.\")\n",
    "#         skipped_images_count += 1\n",
    "#         continue\n",
    "#     gt_result = angle_of_progression_estimation(gt_img, return_img=True)\n",
    "#     pred_result = angle_of_progression_estimation(pred_img, return_img=True)\n",
    "    \n",
    "#     if gt_result is None or pred_result is None:\n",
    "#         # print(f\"Skipping image pair {gt_name} and {pred_name} due to invalid AoP.\")\n",
    "#         skipped_images_count += 1\n",
    "#         continue\n",
    "    \n",
    "#     gtAoP, gtAoPimg = gt_result\n",
    "#     predAoP, predAoPimg = pred_result\n",
    "    \n",
    "#     # Ensure AoP images are valid (non-empty)\n",
    "#     if gtAoPimg is None or predAoPimg is None or gtAoPimg.size == 0 or predAoPimg.size == 0:\n",
    "#         # print(f\"Skipping save due to empty AoP image for {gt_name}.\")\n",
    "#         skipped_images_count += 1\n",
    "#         continue\n",
    "    \n",
    "#     # Save AoP annotated images\n",
    "#     # save_aop_images(gt_name, gtAoPimg, predAoPimg)\n",
    "    \n",
    "#     # Append AoP difference\n",
    "#     meanAoP.append(abs(gtAoP - predAoP))\n",
    "\n",
    "# # Calculate and print the mean AoP\n",
    "# if meanAoP:\n",
    "#     print(f\"Mean AoP: {np.mean(meanAoP)}\")\n",
    "# else:\n",
    "#     print(\"No valid AoP calculations were made.\")\n",
    "\n",
    "# # Print skipped images count\n",
    "# print(f\"Skipped images count: {skipped_images_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20ee3ca0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:26:31.399512Z",
     "iopub.status.busy": "2025-04-08T06:26:31.399153Z",
     "iopub.status.idle": "2025-04-08T06:26:31.402951Z",
     "shell.execute_reply": "2025-04-08T06:26:31.402076Z"
    },
    "papermill": {
     "duration": 0.013875,
     "end_time": "2025-04-08T06:26:31.404382",
     "exception": false,
     "start_time": "2025-04-08T06:26:31.390507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(len(meanAoP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c548be81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:26:31.421415Z",
     "iopub.status.busy": "2025-04-08T06:26:31.421065Z",
     "iopub.status.idle": "2025-04-08T06:26:31.425813Z",
     "shell.execute_reply": "2025-04-08T06:26:31.424926Z"
    },
    "papermill": {
     "duration": 0.014921,
     "end_time": "2025-04-08T06:26:31.427238",
     "exception": false,
     "start_time": "2025-04-08T06:26:31.412317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import cv2  # Import OpenCV\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "\n",
    "# # Select random images\n",
    "\n",
    "# # Select random matching images\n",
    "# num_images = min(5, len(common_images))  # Load up to 5\n",
    "# random_images = random.sample(common_images, num_images)\n",
    "\n",
    "# # Create matched pairs\n",
    "# matched_pairs = [(img, f\"pred_{img}\") for img in random_images]\n",
    "\n",
    "# random_images = random.sample(matched_pairs, num_images)\n",
    "\n",
    "# # Create a figure with 5 rows and 4 columns\n",
    "# fig, axs = plt.subplots(nrows=num_images, ncols=4, figsize=(20, num_images * 4))\n",
    "# skipped_images_count = 0\n",
    "# # Load and send images to function\n",
    "# for idx, (gt_name, pred_name) in enumerate(matched_pairs):\n",
    "#     print(gt_name)\n",
    "#     print(pred_name)\n",
    "\n",
    "#     # Read images with OpenCV\n",
    "#     gt_img = cv2.imread(os.path.join(gt_path, gt_name), 0)  # Load as grayscale\n",
    "#     pred_img = cv2.imread(os.path.join(pred_path, pred_name), 0)  # Load as grayscale\n",
    "    \n",
    "#     # Check if images are loaded properly\n",
    "#     # if gt_img is None or pred_img is None:\n",
    "#     #     print(f\"Error loading images: {gt_name}, {pred_name}\")\n",
    "#     #     continue\n",
    "#      # Convert to torch tensors\n",
    "#     # pred_img = torch.from_numpy(pred_img).long()\n",
    "#     # gt_img = torch.from_numpy(gt_img).long()\n",
    "\n",
    "#     # # Print unique values in gt_img and pred_img\n",
    "#     # print(torch.unique(gt_img))\n",
    "#     # print(torch.unique(pred_img))\n",
    "#     # Resize both images to 224x224\n",
    "#     gt_img = cv2.resize(gt_img, (224, 224), interpolation=cv2.INTER_NEAREST)  # Use INTER_NEAREST for categorical images\n",
    "#     pred_img = cv2.resize(pred_img, (224, 224), interpolation=cv2.INTER_NEAREST)  # Use INTER_NEAREST for categorical images\n",
    "\n",
    "#      # Convert to torch tensors\n",
    "#     pred_img = torch.from_numpy(pred_img).long()\n",
    "#     gt_img = torch.from_numpy(gt_img).long()\n",
    "\n",
    "#     # Print unique values in gt_img and pred_img\n",
    "#     print(torch.unique(gt_img))\n",
    "#     print(torch.unique(pred_img))\n",
    "#     # Skip if the image contains only two unique classes\n",
    "#     if len(torch.unique(gt_img)) == 2 or len(torch.unique(pred_img)) == 2:\n",
    "#         skipped_images_count += 1\n",
    "#         print(f\"Skipping image {gt_name} and {pred_name} as they contain only 2 unique classes.\")\n",
    "#         continue\n",
    "#     # Define mapping: 0 → 0, 76 → 1, 127 → 1, 149 → 2, 255 → 2\n",
    "#     mapping_gt = {0: 0, 76: 1, 127: 1, 149: 2, 255: 2}\n",
    "#     mapping_pred = {0: 0, 127: 1, 255: 2}\n",
    "\n",
    "#     # Apply mapping using np.vectorize\n",
    "#     pred_img = np.vectorize(mapping_pred.get)(pred_img).astype(np.uint8)\n",
    "#     gt_img = np.vectorize(mapping_gt.get)(gt_img).astype(np.uint8)\n",
    "\n",
    "#     # Convert to torch tensors\n",
    "#     pred_img = torch.from_numpy(pred_img).long()\n",
    "#     gt_img = torch.from_numpy(gt_img).long()\n",
    "\n",
    "#     # Print unique values in gt_img and pred_img\n",
    "#     print(torch.unique(gt_img))\n",
    "#     print(torch.unique(pred_img))\n",
    "\n",
    "#     # Ground Truth AoP (Column 0)\n",
    "#     axs[idx, 0].imshow(gt_img.numpy(), cmap='gray')\n",
    "#     axs[idx, 0].set_title(f\"GT img: {gt_name}\")\n",
    "#     axs[idx, 0].axis('off')\n",
    "\n",
    "#     # Predicted Mask AoP (Column 1)\n",
    "#     gtAoP, gtAoPimg = angle_of_progression_estimation(gt_img, return_img=True)\n",
    "#     axs[idx, 1].imshow(gtAoPimg, cmap='gray')\n",
    "#     axs[idx, 1].set_title(f\"GT AoP: {round(gtAoP, 2)}°\")\n",
    "#     axs[idx, 1].axis('off')\n",
    "\n",
    "#     # Ground Truth Mask (Column 2)\n",
    "#     axs[idx, 2].imshow(pred_img.numpy(), cmap='gray')\n",
    "#     axs[idx, 2].set_title(f\"Pred img: {pred_name}\")\n",
    "#     axs[idx, 2].axis('off')\n",
    "\n",
    "#     # Predicted Mask AoP (Column 3)\n",
    "#     predAoP, predAoPimg = angle_of_progression_estimation(pred_img, return_img=True)\n",
    "#     axs[idx, 3].imshow(predAoPimg, cmap='gray')\n",
    "#     axs[idx, 3].set_title(f\"Pred AoP: {round(predAoP, 2)}°\")\n",
    "#     axs[idx, 3].axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# print(skipped_images_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a9c3df",
   "metadata": {
    "papermill": {
     "duration": 0.007467,
     "end_time": "2025-04-08T06:26:31.442600",
     "exception": false,
     "start_time": "2025-04-08T06:26:31.435133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c2093",
   "metadata": {
    "papermill": {
     "duration": 0.007482,
     "end_time": "2025-04-08T06:26:31.457773",
     "exception": false,
     "start_time": "2025-04-08T06:26:31.450291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "712bf9c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:26:31.474999Z",
     "iopub.status.busy": "2025-04-08T06:26:31.474651Z",
     "iopub.status.idle": "2025-04-08T06:26:31.491134Z",
     "shell.execute_reply": "2025-04-08T06:26:31.489926Z"
    },
    "papermill": {
     "duration": 0.027437,
     "end_time": "2025-04-08T06:26:31.492872",
     "exception": false,
     "start_time": "2025-04-08T06:26:31.465435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matching masks: 1414\n",
      "GT: 00793_PSFHAoP4k_mask.png  -->  Pred: pred_00793_PSFHAoP4k_mask.png\n",
      "GT: 04293_PSFHAoP1358_mask.png  -->  Pred: pred_04293_PSFHAoP1358_mask.png\n",
      "GT: 03997_PSFHAoP4k_mask.png  -->  Pred: pred_03997_PSFHAoP4k_mask.png\n",
      "GT: 20190918T120011_49_mask.png  -->  Pred: pred_20190918T120011_49_mask.png\n",
      "GT: 20191214T103803_479_mask.png  -->  Pred: pred_20191214T103803_479_mask.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# Paths\n",
    "gt_path = \"/kaggle/input/ground-truth-mask-img/test_Masks-20250329T151233Z-001/test_Masks\"\n",
    "pred_path = \"/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/unet_1414/Unet/predicted_masks\"\n",
    "\n",
    "# Get filenames\n",
    "gt_files = sorted(os.listdir(gt_path))\n",
    "pred_files = sorted(os.listdir(pred_path))\n",
    "\n",
    "# Remove \"pred_\" from predicted filenames\n",
    "clean_pred_files = [fname.replace(\"pred_\", \"\") for fname in pred_files]\n",
    "\n",
    "# Find common filenames\n",
    "common_images = list(set(gt_files) & set(clean_pred_files))\n",
    "\n",
    "print(f\"Total matching masks: {len(common_images)}\")  # Check how many match\n",
    "\n",
    "# Select random matching images\n",
    "num_images = min(5, len(common_images))  # Load up to 5\n",
    "random_images = random.sample(common_images, num_images)\n",
    "\n",
    "# Create matched pairs\n",
    "matched_pairs = [(img, f\"pred_{img}\") for img in random_images]\n",
    "\n",
    "# Print matched pairs for verification\n",
    "for gt_file, pred_file in matched_pairs:\n",
    "    print(f\"GT: {gt_file}  -->  Pred: {pred_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58a9a363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:26:31.510627Z",
     "iopub.status.busy": "2025-04-08T06:26:31.510231Z",
     "iopub.status.idle": "2025-04-08T06:27:26.193579Z",
     "shell.execute_reply": "2025-04-08T06:27:26.192396Z"
    },
    "papermill": {
     "duration": 54.702046,
     "end_time": "2025-04-08T06:27:26.202971",
     "exception": false,
     "start_time": "2025-04-08T06:26:31.500925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping PS contour due to insufficient points.\n",
      "Mean Pred AoP: 93.63290\n",
      "Mean Diff AoP: 4.33484\n",
      "Median Diff AoP: 1.87167\n",
      "Standard Deviation of Diff AoP: 5.94611\n",
      "Skipped images count: 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2  # Import OpenCV\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Create matched pairs\n",
    "matched_pairs = [(img, f\"pred_{img}\") for img in common_images]\n",
    "\n",
    "meanAoP = []  # For storing the absolute differences\n",
    "mean_pred_aop = []  # For storing predicted AoP values\n",
    "skipped_images_count = 0\n",
    "\n",
    "# Load and send images to function\n",
    "for idx, (gt_name, pred_name) in enumerate(matched_pairs):\n",
    "    # Read images with OpenCV\n",
    "    gt_img = cv2.imread(os.path.join(gt_path, gt_name), 0)  # Load as grayscale\n",
    "    pred_img = cv2.imread(os.path.join(pred_path, pred_name), 0)  # Load as grayscale\n",
    "    \n",
    "    # Check if images are loaded properly\n",
    "    if gt_img is None or pred_img is None:\n",
    "        print(f\"Error loading images: {gt_name}, {pred_name}\")\n",
    "        continue\n",
    "    \n",
    "    gt_img = cv2.resize(gt_img, (224, 224), interpolation=cv2.INTER_NEAREST)  # Use INTER_NEAREST for categorical images\n",
    "    pred_img = cv2.resize(pred_img, (224, 224), interpolation=cv2.INTER_NEAREST)  # Use INTER_NEAREST for categorical images\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    pred_img = torch.from_numpy(pred_img).long()\n",
    "    gt_img = torch.from_numpy(gt_img).long()\n",
    "\n",
    "    # Skip if the image contains only two unique classes\n",
    "    if len(torch.unique(gt_img)) == 2 or len(torch.unique(pred_img)) == 2:\n",
    "        skipped_images_count += 1\n",
    "        continue\n",
    "\n",
    "    # Define mapping: 0 → 0, 76 → 1, 127 → 1, 149 → 2, 255 → 2\n",
    "    mapping_gt = {0: 0, 76: 1, 127: 1, 149: 2, 255: 2}\n",
    "    mapping_pred = {0: 0, 127: 1, 255: 2}\n",
    "\n",
    "    # Apply mapping using np.vectorize\n",
    "    pred_img = np.vectorize(mapping_pred.get)(pred_img).astype(np.uint8)\n",
    "    gt_img = np.vectorize(mapping_gt.get)(gt_img).astype(np.uint8)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    pred_img = torch.from_numpy(pred_img).long()\n",
    "    gt_img = torch.from_numpy(gt_img).long()\n",
    "\n",
    "    # Angle of Progression Estimation\n",
    "    gt_result = angle_of_progression_estimation(gt_img, return_img=True)\n",
    "    pred_result = angle_of_progression_estimation(pred_img, return_img=True)\n",
    "    \n",
    "    # Check if either AoP value is None, and skip if true\n",
    "    if gt_result is None or pred_result is None:\n",
    "        skipped_images_count += 1\n",
    "        continue\n",
    "    \n",
    "    gtAoP, gtAoPimg = gt_result\n",
    "    predAoP, predAoPimg = pred_result\n",
    "    \n",
    "    # Ensure AoP images are valid (non-empty)\n",
    "    if gtAoPimg is None or predAoPimg is None or gtAoPimg.size == 0 or predAoPimg.size == 0:\n",
    "        skipped_images_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Append AoP difference to list\n",
    "    meanAoP.append(abs(gtAoP - predAoP))\n",
    "    \n",
    "    # Append predicted AoP value\n",
    "    mean_pred_aop.append(predAoP)\n",
    "\n",
    "# Calculate statistics\n",
    "if meanAoP:\n",
    "    mean_diff_aop = np.mean(meanAoP)\n",
    "    median_diff_aop = np.median(meanAoP)\n",
    "    std_diff_aop = np.std(meanAoP)\n",
    "    mean_pred_aop_value = np.mean(mean_pred_aop)\n",
    "else:\n",
    "    print(\"No valid AoP calculations were made.\")\n",
    "    mean_diff_aop = median_diff_aop = std_diff_aop = mean_pred_aop_value = None\n",
    "\n",
    "# Print the statistics\n",
    "print(f\"Mean Pred AoP: {mean_pred_aop_value:.5f}\")\n",
    "print(f\"Mean Diff AoP: {mean_diff_aop:.5f}\")\n",
    "print(f\"Median Diff AoP: {median_diff_aop:.5f}\")\n",
    "print(f\"Standard Deviation of Diff AoP: {std_diff_aop:.5f}\")\n",
    "print(f\"Skipped images count: {skipped_images_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d9e39c",
   "metadata": {
    "papermill": {
     "duration": 0.007328,
     "end_time": "2025-04-08T06:27:26.217954",
     "exception": false,
     "start_time": "2025-04-08T06:27:26.210626",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## medT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1ea7832",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:27:26.234780Z",
     "iopub.status.busy": "2025-04-08T06:27:26.234199Z",
     "iopub.status.idle": "2025-04-08T06:27:26.246630Z",
     "shell.execute_reply": "2025-04-08T06:27:26.245661Z"
    },
    "papermill": {
     "duration": 0.022454,
     "end_time": "2025-04-08T06:27:26.248049",
     "exception": false,
     "start_time": "2025-04-08T06:27:26.225595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matching masks: 1414\n",
      "GT: 20200103T102623_713_mask.png  -->  Pred: pred_20200103T102623_713_mask.png\n",
      "GT: 20191212T102143_59_mask.png  -->  Pred: pred_20191212T102143_59_mask.png\n",
      "GT: 00177_PSFHAoP4k_mask.png  -->  Pred: pred_00177_PSFHAoP4k_mask.png\n",
      "GT: 00610_PSFHAoP4k_mask.png  -->  Pred: pred_00610_PSFHAoP4k_mask.png\n",
      "GT: 01913_PSFHAoP4k_mask.png  -->  Pred: pred_01913_PSFHAoP4k_mask.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# Paths\n",
    "gt_path = \"/kaggle/input/ground-truth-mask-img/test_Masks-20250329T151233Z-001/test_Masks\"\n",
    "pred_path = \"/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/medT_1414/medT/predicted_masks\"\n",
    "\n",
    "# Get filenames\n",
    "gt_files = sorted(os.listdir(gt_path))\n",
    "pred_files = sorted(os.listdir(pred_path))\n",
    "\n",
    "# Remove \"pred_\" from predicted filenames\n",
    "clean_pred_files = [fname.replace(\"pred_\", \"\") for fname in pred_files]\n",
    "\n",
    "# Find common filenames\n",
    "common_images = list(set(gt_files) & set(clean_pred_files))\n",
    "\n",
    "print(f\"Total matching masks: {len(common_images)}\")  # Check how many match\n",
    "\n",
    "# Select random matching images\n",
    "num_images = min(5, len(common_images))  # Load up to 5\n",
    "random_images = random.sample(common_images, num_images)\n",
    "\n",
    "# Create matched pairs\n",
    "matched_pairs = [(img, f\"pred_{img}\") for img in random_images]\n",
    "\n",
    "# Print matched pairs for verification\n",
    "for gt_file, pred_file in matched_pairs:\n",
    "    print(f\"GT: {gt_file}  -->  Pred: {pred_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f62f6c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:27:26.265059Z",
     "iopub.status.busy": "2025-04-08T06:27:26.264723Z",
     "iopub.status.idle": "2025-04-08T06:28:08.652357Z",
     "shell.execute_reply": "2025-04-08T06:28:08.651245Z"
    },
    "papermill": {
     "duration": 42.406044,
     "end_time": "2025-04-08T06:28:08.661995",
     "exception": false,
     "start_time": "2025-04-08T06:27:26.255951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Pred AoP: 96.12322\n",
      "Mean Diff AoP: 8.37229\n",
      "Median Diff AoP: 4.73017\n",
      "Standard Deviation of Diff AoP: 10.43178\n",
      "Skipped images count: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2  # Import OpenCV\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Create matched pairs\n",
    "matched_pairs = [(img, f\"pred_{img}\") for img in common_images]\n",
    "\n",
    "meanAoP = []  # For storing the absolute differences\n",
    "mean_pred_aop = []  # For storing predicted AoP values\n",
    "skipped_images_count = 0\n",
    "\n",
    "# Load and send images to function\n",
    "for idx, (gt_name, pred_name) in enumerate(matched_pairs):\n",
    "    # Read images with OpenCV\n",
    "    gt_img = cv2.imread(os.path.join(gt_path, gt_name), 0)  # Load as grayscale\n",
    "    pred_img = cv2.imread(os.path.join(pred_path, pred_name), 0)  # Load as grayscale\n",
    "    \n",
    "    # Check if images are loaded properly\n",
    "    if gt_img is None or pred_img is None:\n",
    "        print(f\"Error loading images: {gt_name}, {pred_name}\")\n",
    "        continue\n",
    "    \n",
    "    gt_img = cv2.resize(gt_img, (224, 224), interpolation=cv2.INTER_NEAREST)  # Use INTER_NEAREST for categorical images\n",
    "    pred_img = cv2.resize(pred_img, (224, 224), interpolation=cv2.INTER_NEAREST)  # Use INTER_NEAREST for categorical images\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    pred_img = torch.from_numpy(pred_img).long()\n",
    "    gt_img = torch.from_numpy(gt_img).long()\n",
    "\n",
    "    # Skip if the image contains only two unique classes\n",
    "    if len(torch.unique(gt_img)) == 2 or len(torch.unique(pred_img)) == 2:\n",
    "        skipped_images_count += 1\n",
    "        continue\n",
    "\n",
    "    # Define mapping: 0 → 0, 76 → 1, 127 → 1, 149 → 2, 255 → 2\n",
    "    mapping_gt = {0: 0, 76: 1, 127: 1, 149: 2, 255: 2}\n",
    "    mapping_pred = {0: 0, 127: 1, 255: 2}\n",
    "\n",
    "    # Apply mapping using np.vectorize\n",
    "    pred_img = np.vectorize(mapping_pred.get)(pred_img).astype(np.uint8)\n",
    "    gt_img = np.vectorize(mapping_gt.get)(gt_img).astype(np.uint8)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    pred_img = torch.from_numpy(pred_img).long()\n",
    "    gt_img = torch.from_numpy(gt_img).long()\n",
    "\n",
    "    # Angle of Progression Estimation\n",
    "    gt_result = angle_of_progression_estimation(gt_img, return_img=True)\n",
    "    pred_result = angle_of_progression_estimation(pred_img, return_img=True)\n",
    "    \n",
    "    # Check if either AoP value is None, and skip if true\n",
    "    if gt_result is None or pred_result is None:\n",
    "        skipped_images_count += 1\n",
    "        continue\n",
    "    \n",
    "    gtAoP, gtAoPimg = gt_result\n",
    "    predAoP, predAoPimg = pred_result\n",
    "    \n",
    "    # Ensure AoP images are valid (non-empty)\n",
    "    if gtAoPimg is None or predAoPimg is None or gtAoPimg.size == 0 or predAoPimg.size == 0:\n",
    "        skipped_images_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Append AoP difference to list\n",
    "    meanAoP.append(abs(gtAoP - predAoP))\n",
    "    \n",
    "    # Append predicted AoP value\n",
    "    mean_pred_aop.append(predAoP)\n",
    "\n",
    "# Calculate statistics\n",
    "if meanAoP:\n",
    "    mean_diff_aop = np.mean(meanAoP)\n",
    "    median_diff_aop = np.median(meanAoP)\n",
    "    std_diff_aop = np.std(meanAoP)\n",
    "    mean_pred_aop_value = np.mean(mean_pred_aop)\n",
    "else:\n",
    "    print(\"No valid AoP calculations were made.\")\n",
    "    mean_diff_aop = median_diff_aop = std_diff_aop = mean_pred_aop_value = None\n",
    "\n",
    "# Print the statistics\n",
    "print(f\"Mean Pred AoP: {mean_pred_aop_value:.5f}\")\n",
    "print(f\"Mean Diff AoP: {mean_diff_aop:.5f}\")\n",
    "print(f\"Median Diff AoP: {median_diff_aop:.5f}\")\n",
    "print(f\"Standard Deviation of Diff AoP: {std_diff_aop:.5f}\")\n",
    "print(f\"Skipped images count: {skipped_images_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f519c",
   "metadata": {
    "papermill": {
     "duration": 0.007564,
     "end_time": "2025-04-08T06:28:08.677915",
     "exception": false,
     "start_time": "2025-04-08T06:28:08.670351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Raunet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ae7c9db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:28:08.695026Z",
     "iopub.status.busy": "2025-04-08T06:28:08.694646Z",
     "iopub.status.idle": "2025-04-08T06:28:08.707388Z",
     "shell.execute_reply": "2025-04-08T06:28:08.706355Z"
    },
    "papermill": {
     "duration": 0.023035,
     "end_time": "2025-04-08T06:28:08.708914",
     "exception": false,
     "start_time": "2025-04-08T06:28:08.685879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matching masks: 1414\n",
      "GT: 00817_PSFHAoP4k_mask.png  -->  Pred: pred_00817_PSFHAoP4k_mask.png\n",
      "GT: 00081_PSFHAoP4k_mask.png  -->  Pred: pred_00081_PSFHAoP4k_mask.png\n",
      "GT: 01733_PSFHAoP4k_mask.png  -->  Pred: pred_01733_PSFHAoP4k_mask.png\n",
      "GT: 20191218T105909_449_mask.png  -->  Pred: pred_20191218T105909_449_mask.png\n",
      "GT: 02963_PSFHAoP4k_mask.png  -->  Pred: pred_02963_PSFHAoP4k_mask.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# Paths\n",
    "gt_path = \"/kaggle/input/ground-truth-mask-img/test_Masks-20250329T151233Z-001/test_Masks\"\n",
    "pred_path = \"/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/RAUnet_1414/RAUnet/predicted_masks\"\n",
    "\n",
    "# Get filenames\n",
    "gt_files = sorted(os.listdir(gt_path))\n",
    "pred_files = sorted(os.listdir(pred_path))\n",
    "\n",
    "# Remove \"pred_\" from predicted filenames\n",
    "clean_pred_files = [fname.replace(\"pred_\", \"\") for fname in pred_files]\n",
    "\n",
    "# Find common filenames\n",
    "common_images = list(set(gt_files) & set(clean_pred_files))\n",
    "\n",
    "print(f\"Total matching masks: {len(common_images)}\")  # Check how many match\n",
    "\n",
    "# Select random matching images\n",
    "num_images = min(5, len(common_images))  # Load up to 5\n",
    "random_images = random.sample(common_images, num_images)\n",
    "\n",
    "# Create matched pairs\n",
    "matched_pairs = [(img, f\"pred_{img}\") for img in random_images]\n",
    "\n",
    "# Print matched pairs for verification\n",
    "for gt_file, pred_file in matched_pairs:\n",
    "    print(f\"GT: {gt_file}  -->  Pred: {pred_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79f937ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:28:08.726599Z",
     "iopub.status.busy": "2025-04-08T06:28:08.726144Z",
     "iopub.status.idle": "2025-04-08T06:28:50.775858Z",
     "shell.execute_reply": "2025-04-08T06:28:50.774721Z"
    },
    "papermill": {
     "duration": 42.068977,
     "end_time": "2025-04-08T06:28:50.785955",
     "exception": false,
     "start_time": "2025-04-08T06:28:08.716978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Pred AoP: 93.88466\n",
      "Mean Diff AoP: 3.92404\n",
      "Median Diff AoP: 2.12935\n",
      "Standard Deviation of Diff AoP: 5.90912\n",
      "Skipped images count: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2  # Import OpenCV\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Create matched pairs\n",
    "matched_pairs = [(img, f\"pred_{img}\") for img in common_images]\n",
    "\n",
    "meanAoP = []  # For storing the absolute differences\n",
    "mean_pred_aop = []  # For storing predicted AoP values\n",
    "skipped_images_count = 0\n",
    "\n",
    "# Load and send images to function\n",
    "for idx, (gt_name, pred_name) in enumerate(matched_pairs):\n",
    "    # Read images with OpenCV\n",
    "    gt_img = cv2.imread(os.path.join(gt_path, gt_name), 0)  # Load as grayscale\n",
    "    pred_img = cv2.imread(os.path.join(pred_path, pred_name), 0)  # Load as grayscale\n",
    "    \n",
    "    # Check if images are loaded properly\n",
    "    if gt_img is None or pred_img is None:\n",
    "        print(f\"Error loading images: {gt_name}, {pred_name}\")\n",
    "        continue\n",
    "    \n",
    "    gt_img = cv2.resize(gt_img, (224, 224), interpolation=cv2.INTER_NEAREST)  # Use INTER_NEAREST for categorical images\n",
    "    pred_img = cv2.resize(pred_img, (224, 224), interpolation=cv2.INTER_NEAREST)  # Use INTER_NEAREST for categorical images\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    pred_img = torch.from_numpy(pred_img).long()\n",
    "    gt_img = torch.from_numpy(gt_img).long()\n",
    "\n",
    "    # Skip if the image contains only two unique classes\n",
    "    if len(torch.unique(gt_img)) == 2 or len(torch.unique(pred_img)) == 2:\n",
    "        skipped_images_count += 1\n",
    "        continue\n",
    "\n",
    "    # Define mapping: 0 → 0, 76 → 1, 127 → 1, 149 → 2, 255 → 2\n",
    "    mapping_gt = {0: 0, 76: 1, 127: 1, 149: 2, 255: 2}\n",
    "    mapping_pred = {0: 0, 127: 1, 255: 2}\n",
    "\n",
    "    # Apply mapping using np.vectorize\n",
    "    pred_img = np.vectorize(mapping_pred.get)(pred_img).astype(np.uint8)\n",
    "    gt_img = np.vectorize(mapping_gt.get)(gt_img).astype(np.uint8)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    pred_img = torch.from_numpy(pred_img).long()\n",
    "    gt_img = torch.from_numpy(gt_img).long()\n",
    "\n",
    "    # Angle of Progression Estimation\n",
    "    gt_result = angle_of_progression_estimation(gt_img, return_img=True)\n",
    "    pred_result = angle_of_progression_estimation(pred_img, return_img=True)\n",
    "    \n",
    "    # Check if either AoP value is None, and skip if true\n",
    "    if gt_result is None or pred_result is None:\n",
    "        skipped_images_count += 1\n",
    "        continue\n",
    "    \n",
    "    gtAoP, gtAoPimg = gt_result\n",
    "    predAoP, predAoPimg = pred_result\n",
    "    \n",
    "    # Ensure AoP images are valid (non-empty)\n",
    "    if gtAoPimg is None or predAoPimg is None or gtAoPimg.size == 0 or predAoPimg.size == 0:\n",
    "        skipped_images_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Append AoP difference to list\n",
    "    meanAoP.append(abs(gtAoP - predAoP))\n",
    "    \n",
    "    # Append predicted AoP value\n",
    "    mean_pred_aop.append(predAoP)\n",
    "\n",
    "# Calculate statistics\n",
    "if meanAoP:\n",
    "    mean_diff_aop = np.mean(meanAoP)\n",
    "    median_diff_aop = np.median(meanAoP)\n",
    "    std_diff_aop = np.std(meanAoP)\n",
    "    mean_pred_aop_value = np.mean(mean_pred_aop)\n",
    "else:\n",
    "    print(\"No valid AoP calculations were made.\")\n",
    "    mean_diff_aop = median_diff_aop = std_diff_aop = mean_pred_aop_value = None\n",
    "\n",
    "# Print the statistics\n",
    "print(f\"Mean Pred AoP: {mean_pred_aop_value:.5f}\")\n",
    "print(f\"Mean Diff AoP: {mean_diff_aop:.5f}\")\n",
    "print(f\"Median Diff AoP: {median_diff_aop:.5f}\")\n",
    "print(f\"Standard Deviation of Diff AoP: {std_diff_aop:.5f}\")\n",
    "print(f\"Skipped images count: {skipped_images_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2a447d",
   "metadata": {
    "papermill": {
     "duration": 0.007875,
     "end_time": "2025-04-08T06:28:50.801963",
     "exception": false,
     "start_time": "2025-04-08T06:28:50.794088",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## transunet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48a12970",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:28:50.819385Z",
     "iopub.status.busy": "2025-04-08T06:28:50.819044Z",
     "iopub.status.idle": "2025-04-08T06:28:50.831364Z",
     "shell.execute_reply": "2025-04-08T06:28:50.830178Z"
    },
    "papermill": {
     "duration": 0.022906,
     "end_time": "2025-04-08T06:28:50.832922",
     "exception": false,
     "start_time": "2025-04-08T06:28:50.810016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matching masks: 1414\n",
      "GT: 00635_PSFHAoP4k_mask.png  -->  Pred: pred_00635_PSFHAoP4k_mask.png\n",
      "GT: 00190_PSFHAoP4k_mask.png  -->  Pred: pred_00190_PSFHAoP4k_mask.png\n",
      "GT: 03959_PSFHAoP1358_mask.png  -->  Pred: pred_03959_PSFHAoP1358_mask.png\n",
      "GT: 03621_PSFHAoP4k_mask.png  -->  Pred: pred_03621_PSFHAoP4k_mask.png\n",
      "GT: 20190918T123342_949_mask.png  -->  Pred: pred_20190918T123342_949_mask.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# Paths\n",
    "gt_path = \"/kaggle/input/ground-truth-mask-img/test_Masks-20250329T151233Z-001/test_Masks\"\n",
    "pred_path = \"/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/TransUnet_1414/TransUnet/predicted_masks\"\n",
    "\n",
    "# Get filenames\n",
    "gt_files = sorted(os.listdir(gt_path))\n",
    "pred_files = sorted(os.listdir(pred_path))\n",
    "\n",
    "# Remove \"pred_\" from predicted filenames\n",
    "clean_pred_files = [fname.replace(\"pred_\", \"\") for fname in pred_files]\n",
    "\n",
    "# Find common filenames\n",
    "common_images = list(set(gt_files) & set(clean_pred_files))\n",
    "\n",
    "print(f\"Total matching masks: {len(common_images)}\")  # Check how many match\n",
    "\n",
    "# Select random matching images\n",
    "num_images = min(5, len(common_images))  # Load up to 5\n",
    "random_images = random.sample(common_images, num_images)\n",
    "\n",
    "# Create matched pairs\n",
    "matched_pairs = [(img, f\"pred_{img}\") for img in random_images]\n",
    "\n",
    "# Print matched pairs for verification\n",
    "for gt_file, pred_file in matched_pairs:\n",
    "    print(f\"GT: {gt_file}  -->  Pred: {pred_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b154f7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:28:50.850620Z",
     "iopub.status.busy": "2025-04-08T06:28:50.850203Z",
     "iopub.status.idle": "2025-04-08T06:29:32.619704Z",
     "shell.execute_reply": "2025-04-08T06:29:32.618642Z"
    },
    "papermill": {
     "duration": 41.788603,
     "end_time": "2025-04-08T06:29:32.629748",
     "exception": false,
     "start_time": "2025-04-08T06:28:50.841145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Pred AoP: 94.30335\n",
      "Mean Diff AoP: 3.83576\n",
      "Median Diff AoP: 2.28318\n",
      "Standard Deviation of Diff AoP: 4.26828\n",
      "Skipped images count: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2  # Import OpenCV\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Create matched pairs\n",
    "matched_pairs = [(img, f\"pred_{img}\") for img in common_images]\n",
    "\n",
    "meanAoP = []  # For storing the absolute differences\n",
    "mean_pred_aop = []  # For storing predicted AoP values\n",
    "skipped_images_count = 0\n",
    "\n",
    "# Load and send images to function\n",
    "for idx, (gt_name, pred_name) in enumerate(matched_pairs):\n",
    "    # Read images with OpenCV\n",
    "    gt_img = cv2.imread(os.path.join(gt_path, gt_name), 0)  # Load as grayscale\n",
    "    pred_img = cv2.imread(os.path.join(pred_path, pred_name), 0)  # Load as grayscale\n",
    "    \n",
    "    # Check if images are loaded properly\n",
    "    if gt_img is None or pred_img is None:\n",
    "        print(f\"Error loading images: {gt_name}, {pred_name}\")\n",
    "        continue\n",
    "    \n",
    "    gt_img = cv2.resize(gt_img, (224, 224), interpolation=cv2.INTER_NEAREST)  # Use INTER_NEAREST for categorical images\n",
    "    pred_img = cv2.resize(pred_img, (224, 224), interpolation=cv2.INTER_NEAREST)  # Use INTER_NEAREST for categorical images\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    pred_img = torch.from_numpy(pred_img).long()\n",
    "    gt_img = torch.from_numpy(gt_img).long()\n",
    "\n",
    "    # Skip if the image contains only two unique classes\n",
    "    if len(torch.unique(gt_img)) == 2 or len(torch.unique(pred_img)) == 2:\n",
    "        skipped_images_count += 1\n",
    "        continue\n",
    "\n",
    "    # Define mapping: 0 → 0, 76 → 1, 127 → 1, 149 → 2, 255 → 2\n",
    "    mapping_gt = {0: 0, 76: 1, 127: 1, 149: 2, 255: 2}\n",
    "    mapping_pred = {0: 0, 127: 1, 255: 2}\n",
    "\n",
    "    # Apply mapping using np.vectorize\n",
    "    pred_img = np.vectorize(mapping_pred.get)(pred_img).astype(np.uint8)\n",
    "    gt_img = np.vectorize(mapping_gt.get)(gt_img).astype(np.uint8)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    pred_img = torch.from_numpy(pred_img).long()\n",
    "    gt_img = torch.from_numpy(gt_img).long()\n",
    "\n",
    "    # Angle of Progression Estimation\n",
    "    gt_result = angle_of_progression_estimation(gt_img, return_img=True)\n",
    "    pred_result = angle_of_progression_estimation(pred_img, return_img=True)\n",
    "    \n",
    "    # Check if either AoP value is None, and skip if true\n",
    "    if gt_result is None or pred_result is None:\n",
    "        skipped_images_count += 1\n",
    "        continue\n",
    "    \n",
    "    gtAoP, gtAoPimg = gt_result\n",
    "    predAoP, predAoPimg = pred_result\n",
    "    \n",
    "    # Ensure AoP images are valid (non-empty)\n",
    "    if gtAoPimg is None or predAoPimg is None or gtAoPimg.size == 0 or predAoPimg.size == 0:\n",
    "        skipped_images_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Append AoP difference to list\n",
    "    meanAoP.append(abs(gtAoP - predAoP))\n",
    "    \n",
    "    # Append predicted AoP value\n",
    "    mean_pred_aop.append(predAoP)\n",
    "\n",
    "# Calculate statistics\n",
    "if meanAoP:\n",
    "    mean_diff_aop = np.mean(meanAoP)\n",
    "    median_diff_aop = np.median(meanAoP)\n",
    "    std_diff_aop = np.std(meanAoP)\n",
    "    mean_pred_aop_value = np.mean(mean_pred_aop)\n",
    "else:\n",
    "    print(\"No valid AoP calculations were made.\")\n",
    "    mean_diff_aop = median_diff_aop = std_diff_aop = mean_pred_aop_value = None\n",
    "\n",
    "# Print the statistics\n",
    "print(f\"Mean Pred AoP: {mean_pred_aop_value:.5f}\")\n",
    "print(f\"Mean Diff AoP: {mean_diff_aop:.5f}\")\n",
    "print(f\"Median Diff AoP: {median_diff_aop:.5f}\")\n",
    "print(f\"Standard Deviation of Diff AoP: {std_diff_aop:.5f}\")\n",
    "print(f\"Skipped images count: {skipped_images_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2e4743",
   "metadata": {
    "papermill": {
     "duration": 0.007906,
     "end_time": "2025-04-08T06:29:32.646141",
     "exception": false,
     "start_time": "2025-04-08T06:29:32.638235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## transfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3f43187",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:29:32.664120Z",
     "iopub.status.busy": "2025-04-08T06:29:32.663718Z",
     "iopub.status.idle": "2025-04-08T06:29:32.677061Z",
     "shell.execute_reply": "2025-04-08T06:29:32.675771Z"
    },
    "papermill": {
     "duration": 0.024473,
     "end_time": "2025-04-08T06:29:32.678868",
     "exception": false,
     "start_time": "2025-04-08T06:29:32.654395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matching masks: 1414\n",
      "GT: 02654_PSFHAoP4k_mask.png  -->  Pred: pred_02654_PSFHAoP4k_mask.png\n",
      "GT: 20190909T161453_219_mask.png  -->  Pred: pred_20190909T161453_219_mask.png\n",
      "GT: 03988_PSFHAoP1358_mask.png  -->  Pred: pred_03988_PSFHAoP1358_mask.png\n",
      "GT: 00675_PSFHAoP4k_mask.png  -->  Pred: pred_00675_PSFHAoP4k_mask.png\n",
      "GT: 02161_PSFHAoP4k_mask.png  -->  Pred: pred_02161_PSFHAoP4k_mask.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# Paths\n",
    "gt_path = \"/kaggle/input/ground-truth-mask-img/test_Masks-20250329T151233Z-001/test_Masks\"\n",
    "pred_path = \"/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/TransFuse_1414/TransFuse/predicted_masks\"\n",
    "\n",
    "# Get filenames\n",
    "gt_files = sorted(os.listdir(gt_path))\n",
    "pred_files = sorted(os.listdir(pred_path))\n",
    "\n",
    "# Remove \"pred_\" from predicted filenames\n",
    "clean_pred_files = [fname.replace(\"pred_\", \"\") for fname in pred_files]\n",
    "\n",
    "# Find common filenames\n",
    "common_images = list(set(gt_files) & set(clean_pred_files))\n",
    "\n",
    "print(f\"Total matching masks: {len(common_images)}\")  # Check how many match\n",
    "\n",
    "# Select random matching images\n",
    "num_images = min(5, len(common_images))  # Load up to 5\n",
    "random_images = random.sample(common_images, num_images)\n",
    "\n",
    "# Create matched pairs\n",
    "matched_pairs = [(img, f\"pred_{img}\") for img in random_images]\n",
    "\n",
    "# Print matched pairs for verification\n",
    "for gt_file, pred_file in matched_pairs:\n",
    "    print(f\"GT: {gt_file}  -->  Pred: {pred_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64cf970b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:29:32.697270Z",
     "iopub.status.busy": "2025-04-08T06:29:32.696888Z",
     "iopub.status.idle": "2025-04-08T06:30:14.610382Z",
     "shell.execute_reply": "2025-04-08T06:30:14.609238Z"
    },
    "papermill": {
     "duration": 41.931872,
     "end_time": "2025-04-08T06:30:14.619293",
     "exception": false,
     "start_time": "2025-04-08T06:29:32.687421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping PS contour due to insufficient points.\n",
      "Mean Pred AoP: 95.20934\n",
      "Mean Diff AoP: 8.51317\n",
      "Median Diff AoP: 2.91189\n",
      "Standard Deviation of Diff AoP: 12.78325\n",
      "Skipped images count: 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2  # Import OpenCV\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Create matched pairs\n",
    "matched_pairs = [(img, f\"pred_{img}\") for img in common_images]\n",
    "\n",
    "meanAoP = []  # For storing the absolute differences\n",
    "mean_pred_aop = []  # For storing predicted AoP values\n",
    "skipped_images_count = 0\n",
    "\n",
    "# Load and send images to function\n",
    "for idx, (gt_name, pred_name) in enumerate(matched_pairs):\n",
    "    # Read images with OpenCV\n",
    "    gt_img = cv2.imread(os.path.join(gt_path, gt_name), 0)  # Load as grayscale\n",
    "    pred_img = cv2.imread(os.path.join(pred_path, pred_name), 0)  # Load as grayscale\n",
    "    \n",
    "    # Check if images are loaded properly\n",
    "    if gt_img is None or pred_img is None:\n",
    "        print(f\"Error loading images: {gt_name}, {pred_name}\")\n",
    "        continue\n",
    "    \n",
    "    gt_img = cv2.resize(gt_img, (224, 224), interpolation=cv2.INTER_NEAREST)  # Use INTER_NEAREST for categorical images\n",
    "    pred_img = cv2.resize(pred_img, (224, 224), interpolation=cv2.INTER_NEAREST)  # Use INTER_NEAREST for categorical images\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    pred_img = torch.from_numpy(pred_img).long()\n",
    "    gt_img = torch.from_numpy(gt_img).long()\n",
    "\n",
    "    # Skip if the image contains only two unique classes\n",
    "    if len(torch.unique(gt_img)) == 2 or len(torch.unique(pred_img)) == 2:\n",
    "        skipped_images_count += 1\n",
    "        continue\n",
    "\n",
    "    # Define mapping: 0 → 0, 76 → 1, 127 → 1, 149 → 2, 255 → 2\n",
    "    mapping_gt = {0: 0, 76: 1, 127: 1, 149: 2, 255: 2}\n",
    "    mapping_pred = {0: 0, 127: 1, 255: 2}\n",
    "\n",
    "    # Apply mapping using np.vectorize\n",
    "    pred_img = np.vectorize(mapping_pred.get)(pred_img).astype(np.uint8)\n",
    "    gt_img = np.vectorize(mapping_gt.get)(gt_img).astype(np.uint8)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    pred_img = torch.from_numpy(pred_img).long()\n",
    "    gt_img = torch.from_numpy(gt_img).long()\n",
    "\n",
    "    # Angle of Progression Estimation\n",
    "    gt_result = angle_of_progression_estimation(gt_img, return_img=True)\n",
    "    pred_result = angle_of_progression_estimation(pred_img, return_img=True)\n",
    "    \n",
    "    # Check if either AoP value is None, and skip if true\n",
    "    if gt_result is None or pred_result is None:\n",
    "        skipped_images_count += 1\n",
    "        continue\n",
    "    \n",
    "    gtAoP, gtAoPimg = gt_result\n",
    "    predAoP, predAoPimg = pred_result\n",
    "    \n",
    "    # Ensure AoP images are valid (non-empty)\n",
    "    if gtAoPimg is None or predAoPimg is None or gtAoPimg.size == 0 or predAoPimg.size == 0:\n",
    "        skipped_images_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Append AoP difference to list\n",
    "    meanAoP.append(abs(gtAoP - predAoP))\n",
    "    \n",
    "    # Append predicted AoP value\n",
    "    mean_pred_aop.append(predAoP)\n",
    "\n",
    "# Calculate statistics\n",
    "if meanAoP:\n",
    "    mean_diff_aop = np.mean(meanAoP)\n",
    "    median_diff_aop = np.median(meanAoP)\n",
    "    std_diff_aop = np.std(meanAoP)\n",
    "    mean_pred_aop_value = np.mean(mean_pred_aop)\n",
    "else:\n",
    "    print(\"No valid AoP calculations were made.\")\n",
    "    mean_diff_aop = median_diff_aop = std_diff_aop = mean_pred_aop_value = None\n",
    "\n",
    "# Print the statistics\n",
    "print(f\"Mean Pred AoP: {mean_pred_aop_value:.5f}\")\n",
    "print(f\"Mean Diff AoP: {mean_diff_aop:.5f}\")\n",
    "print(f\"Median Diff AoP: {median_diff_aop:.5f}\")\n",
    "print(f\"Standard Deviation of Diff AoP: {std_diff_aop:.5f}\")\n",
    "print(f\"Skipped images count: {skipped_images_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aacfe1",
   "metadata": {
    "papermill": {
     "duration": 0.008071,
     "end_time": "2025-04-08T06:30:14.635876",
     "exception": false,
     "start_time": "2025-04-08T06:30:14.627805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## attentionUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81890493",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:30:14.654311Z",
     "iopub.status.busy": "2025-04-08T06:30:14.653975Z",
     "iopub.status.idle": "2025-04-08T06:30:14.666927Z",
     "shell.execute_reply": "2025-04-08T06:30:14.665893Z"
    },
    "papermill": {
     "duration": 0.024127,
     "end_time": "2025-04-08T06:30:14.668743",
     "exception": false,
     "start_time": "2025-04-08T06:30:14.644616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matching masks: 1414\n",
      "GT: 20191214T103803_649_mask.png  -->  Pred: pred_20191214T103803_649_mask.png\n",
      "GT: 20191008T112326_109_mask.png  -->  Pred: pred_20191008T112326_109_mask.png\n",
      "GT: 20200103T102728_429_mask.png  -->  Pred: pred_20200103T102728_429_mask.png\n",
      "GT: 02230_PSFHAoP4k_mask.png  -->  Pred: pred_02230_PSFHAoP4k_mask.png\n",
      "GT: 00053_PSFHAoP4k_mask.png  -->  Pred: pred_00053_PSFHAoP4k_mask.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# Paths\n",
    "gt_path = \"/kaggle/input/ground-truth-mask-img/test_Masks-20250329T151233Z-001/test_Masks\"\n",
    "pred_path = \"/kaggle/input/final-masks/Pred_Images_for_TN_1414-20250328T162906Z-001/Pred_Images_for_TN_1414/AttentionUnet_1414/AttentionUnet/predicted_masks\"\n",
    "\n",
    "# Get filenames\n",
    "gt_files = sorted(os.listdir(gt_path))\n",
    "pred_files = sorted(os.listdir(pred_path))\n",
    "\n",
    "# Remove \"pred_\" from predicted filenames\n",
    "clean_pred_files = [fname.replace(\"pred_\", \"\") for fname in pred_files]\n",
    "\n",
    "# Find common filenames\n",
    "common_images = list(set(gt_files) & set(clean_pred_files))\n",
    "\n",
    "print(f\"Total matching masks: {len(common_images)}\")  # Check how many match\n",
    "\n",
    "# Select random matching images\n",
    "num_images = min(5, len(common_images))  # Load up to 5\n",
    "random_images = random.sample(common_images, num_images)\n",
    "\n",
    "# Create matched pairs\n",
    "matched_pairs = [(img, f\"pred_{img}\") for img in random_images]\n",
    "\n",
    "# Print matched pairs for verification\n",
    "for gt_file, pred_file in matched_pairs:\n",
    "    print(f\"GT: {gt_file}  -->  Pred: {pred_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "336adc19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:30:14.687431Z",
     "iopub.status.busy": "2025-04-08T06:30:14.687098Z",
     "iopub.status.idle": "2025-04-08T06:30:56.232320Z",
     "shell.execute_reply": "2025-04-08T06:30:56.231274Z"
    },
    "papermill": {
     "duration": 41.565496,
     "end_time": "2025-04-08T06:30:56.242979",
     "exception": false,
     "start_time": "2025-04-08T06:30:14.677483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Pred AoP: 94.75545\n",
      "Mean Diff AoP: 3.83096\n",
      "Median Diff AoP: 1.82738\n",
      "Standard Deviation of Diff AoP: 4.99525\n",
      "Skipped images count: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2  # Import OpenCV\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Create matched pairs\n",
    "matched_pairs = [(img, f\"pred_{img}\") for img in common_images]\n",
    "\n",
    "meanAoP = []  # For storing the absolute differences\n",
    "mean_pred_aop = []  # For storing predicted AoP values\n",
    "skipped_images_count = 0\n",
    "\n",
    "# Load and send images to function\n",
    "for idx, (gt_name, pred_name) in enumerate(matched_pairs):\n",
    "    # Read images with OpenCV\n",
    "    gt_img = cv2.imread(os.path.join(gt_path, gt_name), 0)  # Load as grayscale\n",
    "    pred_img = cv2.imread(os.path.join(pred_path, pred_name), 0)  # Load as grayscale\n",
    "    \n",
    "    # Check if images are loaded properly\n",
    "    if gt_img is None or pred_img is None:\n",
    "        print(f\"Error loading images: {gt_name}, {pred_name}\")\n",
    "        continue\n",
    "    \n",
    "    gt_img = cv2.resize(gt_img, (224, 224), interpolation=cv2.INTER_NEAREST)  # Use INTER_NEAREST for categorical images\n",
    "    pred_img = cv2.resize(pred_img, (224, 224), interpolation=cv2.INTER_NEAREST)  # Use INTER_NEAREST for categorical images\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    pred_img = torch.from_numpy(pred_img).long()\n",
    "    gt_img = torch.from_numpy(gt_img).long()\n",
    "\n",
    "    # Skip if the image contains only two unique classes\n",
    "    if len(torch.unique(gt_img)) == 2 or len(torch.unique(pred_img)) == 2:\n",
    "        skipped_images_count += 1\n",
    "        continue\n",
    "\n",
    "    # Define mapping: 0 → 0, 76 → 1, 127 → 1, 149 → 2, 255 → 2\n",
    "    mapping_gt = {0: 0, 76: 1, 127: 1, 149: 2, 255: 2}\n",
    "    mapping_pred = {0: 0, 127: 1, 255: 2}\n",
    "\n",
    "    # Apply mapping using np.vectorize\n",
    "    pred_img = np.vectorize(mapping_pred.get)(pred_img).astype(np.uint8)\n",
    "    gt_img = np.vectorize(mapping_gt.get)(gt_img).astype(np.uint8)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    pred_img = torch.from_numpy(pred_img).long()\n",
    "    gt_img = torch.from_numpy(gt_img).long()\n",
    "\n",
    "    # Angle of Progression Estimation\n",
    "    gt_result = angle_of_progression_estimation(gt_img, return_img=True)\n",
    "    pred_result = angle_of_progression_estimation(pred_img, return_img=True)\n",
    "    \n",
    "    # Check if either AoP value is None, and skip if true\n",
    "    if gt_result is None or pred_result is None:\n",
    "        skipped_images_count += 1\n",
    "        continue\n",
    "    \n",
    "    gtAoP, gtAoPimg = gt_result\n",
    "    predAoP, predAoPimg = pred_result\n",
    "    \n",
    "    # Ensure AoP images are valid (non-empty)\n",
    "    if gtAoPimg is None or predAoPimg is None or gtAoPimg.size == 0 or predAoPimg.size == 0:\n",
    "        skipped_images_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Append AoP difference to list\n",
    "    meanAoP.append(abs(gtAoP - predAoP))\n",
    "    \n",
    "    # Append predicted AoP value\n",
    "    mean_pred_aop.append(predAoP)\n",
    "\n",
    "# Calculate statistics\n",
    "if meanAoP:\n",
    "    mean_diff_aop = np.mean(meanAoP)\n",
    "    median_diff_aop = np.median(meanAoP)\n",
    "    std_diff_aop = np.std(meanAoP)\n",
    "    mean_pred_aop_value = np.mean(mean_pred_aop)\n",
    "else:\n",
    "    print(\"No valid AoP calculations were made.\")\n",
    "    mean_diff_aop = median_diff_aop = std_diff_aop = mean_pred_aop_value = None\n",
    "\n",
    "# Print the statistics\n",
    "print(f\"Mean Pred AoP: {mean_pred_aop_value:.5f}\")\n",
    "print(f\"Mean Diff AoP: {mean_diff_aop:.5f}\")\n",
    "print(f\"Median Diff AoP: {median_diff_aop:.5f}\")\n",
    "print(f\"Standard Deviation of Diff AoP: {std_diff_aop:.5f}\")\n",
    "print(f\"Skipped images count: {skipped_images_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6533270,
     "sourceId": 10559791,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6596355,
     "sourceId": 10652476,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6992867,
     "sourceId": 11200025,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6998126,
     "sourceId": 11207597,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 391.137338,
   "end_time": "2025-04-08T06:30:57.776033",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-08T06:24:26.638695",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
